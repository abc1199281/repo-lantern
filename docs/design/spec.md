# Lantern Specification

> **Lantern — Your repository mentor.**

## 1. Core Vision

Lantern is a CLI-based repository analysis tool. Its purpose is not simply "generating docs" but guiding developers to deeply understand an unfamiliar codebase with minimal cognitive load, using psychology-driven design and structured decomposition.

### 1.1 Psychological Design Principles

- **Chunking (Miller's Law)**: Each batch processes at most 3 related files to prevent information overload.
- **Scaffolding**: Plan first, review, then execute step-by-step — building a stable ladder for understanding complex systems.
- **Native Language Priming**: Final output is translated into the user's native language, eliminating the cognitive overhead of reading foreign-language technical docs.

### 1.2 LLM Backend Strategy

Lantern supports multiple backend types via a unified `Backend` protocol:

| Backend | Use Case |
| :--- | :--- |
| **Ollama** | Local models, privacy, zero cost |
| **OpenAI** | Production recommended, stable and cost-effective |
| **OpenRouter** | Multi-provider access (Claude, Gemini, etc.) |
| **CLI** | Agent-based workflow for CLI tools (codex, llm, claude) |

Configuration is in `.lantern/lantern.toml` under `[backend]`. See README for setup details.

---

## 2. Document Specifications

### `lantern_plan.md`

The analysis plan generated by the Architect, designed for human review.

Structure:
- **Output Structure Preview**: Shows the expected doc tree under `.lantern/output/`
- **Learning Objectives**: Per-phase goals with batch file lists
- **Dependency Graph**: Mermaid diagram of module relationships
- **Low Confidence Decisions**: Items flagged for human review

### `.lantern/sense/*.sense`

Per-batch analysis records stored as JSON.

Naming: `batch_{N:03d}.sense`

Fields:
- `batch_id` — Batch identifier
- `files` — List of analyzed file paths
- `summary` — Batch-level summary
- `key_insights` — Important patterns and decisions

### Output directory layout

```
.lantern/output/
├── {lang}/
│   ├── top_down/
│   │   ├── OVERVIEW.md
│   │   ├── ARCHITECTURE.md
│   │   ├── CONCEPTS.md
│   │   └── GETTING_STARTED.md
│   └── bottom_up/
│       └── (mirrors repo structure, one .md per source file)
```

---

## 3. CLI Commands

| Mode | Commands | Use Case |
| :--- | :--- | :--- |
| **Simple** | `lantern run` | Quick analysis with defaults |
| **Advanced** | `lantern init` → `lantern plan` → `lantern run` | Review/edit plan before execution |

Config priority (high to low):
1. CLI arguments (`--backend`, `--output`, `--lang`)
2. Project config (`.lantern/lantern.toml`)
3. User config (`~/.config/lantern/lantern.toml`)
4. Defaults

---

## 4. Workflow

1. **Init**: Set up `.lantern/` directory and configuration.
2. **Static Scan**: Build dependency graph using AST / regex parsers.
3. **Planning**: Architect generates `lantern_plan.md` from the dependency graph.
4. **Human Review**: User inspects batch groupings, dependency graph, and low-confidence decisions. Can approve, reject, or edit the plan.
5. **Execution**: Runner processes batches sequentially, generating `.sense` records and bottom-up docs. Temporal RAG carries context across batches.
6. **Synthesis**: Synthesizer reads all `.sense` files and generates top-down docs (OVERVIEW, ARCHITECTURE, CONCEPTS, GETTING_STARTED).

Three operating modes are available:
- **Static planning** (default)
- **Agentic planning** (`--planning-mode agentic`)
- **Full LangGraph workflow** (`--workflow`) with checkpointing and resume

---

## 5. Competitive Analysis

| Tool | Focus | Lantern Differentiator |
| :--- | :--- | :--- |
| **NotebookLM** | AI doc Q&A | Structured output, local support (Ollama), auto Mermaid diagrams |
| **Aider / Cursor** | Code editing | Focused on *understanding*, not editing; dual top-down + bottom-up views |
| **Autodoc / Sphinx** | Doc generation | AI-driven reasoning and architecture narratives, not just docstring extraction |
| **RepoMap** | Relationship visualization | Guided learning narratives + sequence diagrams, not just maps |

### Core Advantages

1. **Auto visualization**: Mermaid flowcharts and sequence diagrams generated per file
2. **Privacy**: Full local execution via Ollama
3. **Cost transparency**: Pre-run estimates, live tracking, online pricing
4. **Psychology-driven**: Batch sizing based on cognitive science
5. **Native language output**: Technical docs in the user's native language
6. **Production-grade reliability**: Checkpoint resume, structured output validation
