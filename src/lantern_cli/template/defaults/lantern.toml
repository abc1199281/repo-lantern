# Lantern Configuration

[lantern]
language = "zh-TW"
output_dir = ".lantern"

[filter]
exclude = [
    "**/__pycache__/*", 
    "**/.git/*", 
    "**/node_modules/*", 
    "**/.venv/*", 
    "**/.idea/*", 
    "**/.vscode/*",
    "**/build*/*"
]

# Backend configuration - choose one:

# Option 1: Ollama (Local, Free, Private)
# [backend]
# type = "ollama"
# ollama_model = "qwen2.5:14b"  # or llama3, mistral, etc.
# # ollama_url = "http://localhost:11434"

# Option 2: OpenAI (Production, Recommended)
[backend]
type = "openai"
openai_model = "gpt-4o-mini"  # Fast and cheap ($0.15/1M input, $0.60/1M output)
# max_output_tokens = 16384  # Increase if you see truncation errors
# openai_model = "gpt-4o"      # Higher quality ($2.50/1M input, $10/1M output)
# openai_api_key_env = "OPENAI_API_KEY"  # Set: export OPENAI_API_KEY=<your-key>

# Option 3: OpenRouter (Multi-model Access)
# [backend]
# type = "openrouter"
# openrouter_model = "openai/gpt-4o-mini"  # or anthropic/claude-sonnet-4, etc.
# # openrouter_url = "https://openrouter.ai/api/v1"
# # openrouter_api_key_env = "OPENROUTER_API_KEY"  # Set: export OPENROUTER_API_KEY=<your-key>

# Option 4: CLI Tool (Use any CLI that accepts stdin and outputs to stdout)
# [backend]
# type = "cli"
# cli_command = "codex exec"  # or "llm -m gpt-4o-mini", etc.
# cli_model_name = "cli"

# LangSmith Observability (optional)
# Uncomment to enable LangSmith tracing for all LangChain/LangGraph calls.
# Requires: export LANGCHAIN_API_KEY=<your-langsmith-api-key>
#[langsmith]
#enabled = true
#project = "repo-lantern"            # Project name in LangSmith dashboard
# endpoint = "https://api.smith.langchain.com"  # Default endpoint
# api_key_env = "LANGCHAIN_API_KEY"             # Env var holding the API key
